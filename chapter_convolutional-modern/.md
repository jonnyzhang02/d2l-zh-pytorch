<!--
 * @Author       : JonnyZhang 71881972+jonnyzhang02@users.noreply.github.com
 * @LastEditTime : 2023-07-24 20:52
 * @FilePath     : \d2l-zh-pytorch\chapter_convolutional-modern\.md
 * 
 * coded by ZhangYang@BUPT, my email is zhangynag0207@bupt.edu.cn
-->
# 现代卷积神经网络

## AlexNet

输入(batch_size, 3, 224, 224)

下面是batch_size=1

![](assets/2023-07-24-17-26-09.png)

[论文](https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)

### 背景

（2012年）

2000年初期的主流算法是核方法。

![](assets/2023-07-24-15-43-33.png)

![](assets/2023-07-24-15-45-26.png)

10-15年前：

![](assets/2023-07-24-15-48-06.png)

计算和数据增长很快。计算增长更快。

![](assets/2023-07-24-15-53-20.png)

### 模型

![](assets/2023-07-24-15-58-04.png)

改变了人们的观念，不再由人提取特征。

量变引起质变。

![](assets/2023-07-24-16-03-40.png)

![](assets/2023-07-24-16-07-20.png)

![](assets/2023-07-24-16-09-11.png)

![](assets/2023-07-24-16-11-21.png)

总结

![](assets/2023-07-24-16-17-42.png)

### AlexNet代码

[notebook](./alexnet.ipynb)


## 使用块的网络VGG

### VGG模型

（2013年）

AlexNet的问题：长得太丑了。长得不规则。

![](assets/2023-07-24-17-29-17.png)

![](assets/2023-07-24-17-30-21.png)

VGG的思想：将卷积层组合为块。

![](assets/2023-07-24-17-32-36.png)

更多的3*3好于 5\*5 ,深但窄。

![](assets/2023-07-24-17-33-44.png)

![](assets/2023-07-24-17-34-17.png)

### VGG代码

VGG是一个很贵的网络，因为它的计算量很大。

[notebook](./vgg.ipynb)

## NiN(网络中的网络)

### NiN模型

用得不多。但提出了一些重要概念。

![](assets/2023-07-24-18-11-02.png)

使用1*1卷积层，起到全连接层的作用。

![](assets/2023-07-24-18-30-01.png)

混合通道。

![](assets/2023-07-24-18-38-58.png)

![](assets/2023-07-24-18-40-27.png)

![](assets/2023-07-24-18-46-15.png)

### NIN代码

[notebook](./nin.ipynb)

## GoogLeNet

GoogLeNet很好地使用了1*1卷积层。

第一个超过100层的卷积神经网络。

![](assets/2023-07-24-20-11-24.png)

### Inception块

![](assets/2023-07-24-20-12-13.png)

输出的通道数是所有分支输出通道数之和。

但**不改变高和宽**。

![](assets/2023-07-24-20-15-27.png)

作者从来没说过这些通道数是怎么来的。

这味药抓一点，那味药抓一点，最后混合在一起。

![](assets/2023-07-24-20-20-40.png)

### GoogLeNet模型

![](assets/2023-07-24-20-22-24.png)

#### 段1-2

![](assets/2023-07-24-20-25-40.png)

把通道数拉上去，把高宽减下来。

```mermaid

graph TD

A[3,224,224] -->|7*7 Conv 64 stride 2 pad 3| B[64,112,112]
B --> |3*3 MaxPool| C[64,56,56]
C --> |1*1 Conv 64| D[64,56,56]
D --> |3*3 Conv 192 pad 1| E[192,56,56]
E --> |3*3 MaxPool stride 2 pad 1| F[192,28,28]

```

#### 段3

![](assets/2023-07-24-20-42-56.png)

#### 段4-5

![](assets/2023-07-24-20-45-18.png)

### 后续版本

![](assets/2023-07-24-20-47-11.png)

V3 效果比较好。还是被经常使用。

### GoogLeNet代码

[notebook](./googlenet.ipynb)

## 批量归一化 Batch Normalization






