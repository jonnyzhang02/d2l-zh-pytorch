<!--
 * @Author       : JonnyZhang 71881972+jonnyzhang02@users.noreply.github.com
 * @LastEditTime : 2023-07-18 11:13
 * @FilePath     : \d2l-zh-pytorch\chapter_linear-networks\.md
 * 
 * coded by ZhangYang@BUPT, my email is zhangynag0207@bupt.edu.cn
-->
# 线性回归

## 1. [线性回归](./linear-regression.ipynb)

![](assets/2023-07-18-10-28-05.png)

![](assets/2023-07-18-10-28-44.png)

![](assets/2023-07-18-10-29-37.png)

![](assets/2023-07-18-10-36-04.png)

![](assets/2023-07-18-10-38-09.png)

唯一有最优解的模型是线性模型，因为它是一个凸函数，而凸函数的局部最优解也是全局最优解。

## [基础优化算法]()

![](assets/2023-07-18-10-40-49.png)

![](assets/2023-07-18-10-45-13.png)

用一部分数据的平均损失来估计总体损失，这叫做**小批量随机梯度下降**。最稳定也最简单，默认的损失计算方法。

## [从零开始实现](./linear-regression-scratch.ipynb)

在PyTorch中，有几种方法可以进行矩阵相乘操作。以下是其中几种常见的方法：

### 1. `torch.matmul`函数

`torch.matmul`: 这是PyTorch提供的主要矩阵相乘函数。它可以用于两个张量（可以是标量、向量、矩阵或多维张量）之间的相乘操作。`torch.matmul`函数**可以处理不同形状的输入张量，自动执行广播和批量操作**。对于二维矩阵相乘，可以使用该函数，如`torch.matmul(tensor1, tensor2)`。

`torch.matmul`函数执行的是矩阵相乘运算，根据输入张量的形状和维度的不同，它可以进行以下几种类型的矩阵相乘运算：

1. **两个二维矩阵的相乘**：如果两个输入张量都是二维矩阵，例如形状为 `(m, n)` 和 `(n, p)`，则 `torch.matmul` 函数会执行标准的矩阵乘法运算，输出一个形状为 `(m, p)` 的结果矩阵。

2. **矩阵和向量的相乘**：如果其中一个输入张量是二维矩阵，另一个是**一维张量**，例如形状为 `(m, n)` 的矩阵和形状为 `(n,)` 的向量，则 `torch.matmul` 函数**会将向量视为列向量，将矩阵看成是行向量的集合**，执行矩阵乘法运算，**输出一个形状为 `(m,)` 的结果向量**。

3. **批量矩阵相乘**：如果输入张量的形状具有批次维度（batch dimension），例如形状为 `(batch_size, m, n)` 和 `(batch_size, n, p)`，则 `torch.matmul` 函数会在批次维度上执行矩阵乘法运算，即对每个批次中的矩阵进行相乘操作。输出的结果张量形状为 `(batch_size, m, p)`。

4. **张量广播和批量矩阵相乘**：如果输入张量的形状可以通过广播（broadcasting）扩展为匹配的形状，例如形状为 `(m, n)` 和 `(n, p)`，则 `torch.matmul` 函数会自动执行广播操作，并在批次维度上执行矩阵乘法运算。输出的结果张量形状为 `(m, p)`。

需要注意的是，`torch.matmul`函数并不执行逐元素乘法（element-wise multiplication），而是执行矩阵乘法运算。如果需要执行逐元素乘法，可以使用`*`运算符。


#### 具体执行过程

**属于上面的情况二**

当你执行 `torch.matmul` 运算符来计算一个形状为 `torch.Size([1000, 2])` 的张量和一个形状为 `tensor([2.0000, -3.4000])` 的张量时，PyTorch 会自动进行广播操作，将第二个张量广播到与第一个张量的形状匹配。

首先，PyTorch 将形状为 `torch.Size([1000, 2])` 的张量视为一个包含 1000 个形状为 `(2,)` 的向量组成的矩阵。然后，它会执行矩阵乘法运算，将这个矩阵和形状为 `(2,)` 的向量进行相乘。

具体过程如下：

1. 将形状为 `torch.Size([1000, 2])` 的张量视为一个包含 1000 个形状为 `(2,)` 的向量组成的矩阵。

2. 将形状为 `(2,)` 的向量视为列向量，得到一个形状为 `(2, 1)` 的矩阵。

3. 执行矩阵乘法运算，将形状为 `torch.Size([1000, 2])` 的矩阵和形状为 `(2, 1)` 的矩阵相乘。

4. 输出结果为一个形状为 `torch.Size([1000, 1])` 的张量，即一个包含 1000 个形状为 `(1,)` 的向量组成的矩阵。

简而言之，`torch.matmul` 运算符会将第二个张量广播到与第一个张量的形状匹配，并执行矩阵乘法运算。在本例中，输出结果将是一个形状为 `torch.Size([1000, 1])` 的张量。

### 2. 其他矩阵相乘函数

2. `torch.mm`: 这是一个专门用于执行**两个二维矩阵相乘**的函数。它要求输入张量是二维的，并且对于相乘的矩阵形状必须满足**内积维度的要求**。可以使用`torch.mm(tensor1, tensor2)`进行二维矩阵相乘操作。

3. `torch.dot`: 这是用于计算两个一维张量的点积的函数。它要求输入张量是一维的，并且长度必须相等。可以使用`torch.dot(tensor1, tensor2)`进行一维张量的点积操作。

4. `*` 运算符：在PyTorch中，使用`*`运算符进行矩阵相乘会执行逐元素乘法（element-wise multiplication），也称为Hadamard积。这意味着对应位置的元素相乘得到的结果组成了新的张量，要求两个张量的形状必须相同。可以使用`result = tensor1 * tensor2`进行逐元素乘法操作。

总结一下，`torch.matmul`和`torch.mm`用于矩阵相乘操作，`torch.matmul`更通用，能够处理多种形状的输入张量。`torch.dot`用于计算一维张量的点积。而`*`运算符用于执行逐元素乘法操作。

