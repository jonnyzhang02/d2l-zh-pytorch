## 计算图

PyTorch中的梯度下降通过计算图实现，其中计算图是一个用于描述计算过程的有向无环图(Directed Acyclic Graph, DAG)。在计算图中，节点表示操作（例如加法、乘法等），边表示数据流动。梯度下降使用计算图来自动计算损失函数对于模型参数的梯度，从而更新参数以最小化损失函数。

假设我们有一个损失函数$L(\theta)$，其中$\theta$表示模型的参数。我们的目标是找到使得$L(\theta)$最小化的$\theta$。梯度下降使用链式法则来计算梯度，该法则允许我们将损失函数的梯度表示为模型参数的梯度。

首先，我们定义计算图中的两类节点：

1. 叶子节点（Leaf Nodes）：这些节点表示输入数据或模型的参数。在梯度下降中，我们通常将输入数据和模型参数看作是叶子节点。

2. 非叶子节点（Non-leaf Nodes）：这些节点表示操作，如加法、乘法、激活函数等。非叶子节点的值由其父节点的值计算得出。

现在，假设我们的计算图如下：

```
        [Input] --- (Operation1) --- (Operation2) --- ... --- [Loss]
           |              |               |
       [Parameter1]  [Parameter2]   [Parameter3]
```

其中，[Input]是输入数据，[Loss]是损失函数，(Operation1)、(Operation2)等表示模型的操作，[Parameter1]、[Parameter2]、[Parameter3]是模型的参数。

我们的目标是找到最优的模型参数[Parameter1]、[Parameter2]、[Parameter3]，以使得损失函数[Loss]最小化。

梯度下降的步骤如下：

1. 前向传播：从[Input]开始，按照计算图的方向依次进行计算，直到得到[Loss]的值。这个过程即为前向传播，也就是模型的正向计算。

2. 反向传播：**在前向传播的过程中，PyTorch会自动构建计算图，并记录每个节点的梯度函数。然后，通过链式法则，从[Loss]节点开始沿着计算图的反向计算每个节点的梯度。**这个过程即为反向传播，也就是计算梯度。

3. 更新参数：在得到每个参数的梯度后，我们可以使用梯度下降算法来更新模型的参数，使得损失函数减小。例如，对于参数$\theta_i$，更新规则可以表示为：$\theta_i = \theta_i - \alpha \cdot \frac{\partial L(\theta)}{\partial \theta_i}$，其中$\alpha$是学习率，控制着每次更新的步长。

4. 重复步骤1-3：重复进行前向传播、反向传播和参数更新，直到达到满足停止条件的迭代次数或达到收敛。

通过这种方式，梯度下降不断更新参数，使得损失函数越来越小，直到达到一个满意的模型性能。

注意：上述步骤中只是简要描述了梯度下降的过程。在实际中，还会结合优化技巧，如随机梯度下降（SGD）、动量法、Adam等来加速优化过程和避免陷入局部最优点。

## 卷积和池化的大小

输出尺寸 = 1 + 步数
这个一是没走的时候的输出
**步数 = （输入尺寸 + 2 * padding - 卷积核尺寸）/ 步长**    **向下取整**
输入尺寸加上padding之后是实际输入
减去卷积核尺寸是卷积核能走的**路程**（以右侧/下侧为准）
除以步长是卷积核走的**步数**，要向下取整